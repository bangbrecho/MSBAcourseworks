{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOM8LrcjGAUs2PWRlnOlgVo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# EDA"],"metadata":{"id":"2-jHSxJ637RE"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"5P5K-ysD3v0k","executionInfo":{"status":"ok","timestamp":1752143708046,"user_tz":-60,"elapsed":2801,"user":{"displayName":"Basil Basil","userId":"10665298970573720638"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","orders=pd.read_csv('olist_orders_dataset.csv')\n","items=pd.read_csv('olist_order_items_dataset.csv')\n","product=pd.read_csv('olist_products_dataset.csv')\n","cust=pd.read_csv('olist_customers_dataset.csv')\n","reviews=pd.read_csv('olist_order_reviews_dataset.csv', encoding='latin1')\n","payment=pd.read_csv('olist_order_payments_dataset.csv')\n","sellers = pd.read_csv(\"olist_sellers_dataset.csv\")"]},{"cell_type":"code","source":["#join data\n","orders=orders.merge(cust, how='left', left_on='customer_id', right_on='customer_id')\n","orders=orders.merge(reviews, how='left', left_on='order_id', right_on='order_id')\n","orders=orders.merge(payment, how='left', left_on='order_id', right_on='order_id')\n","items=items.merge(product, how='left', on='product_id')\n","\n","\n","items=items.merge(sellers, how='left', on='seller_id')\n","basedata=items.merge(orders, how='left', left_on='order_id', right_on='order_id')\n","\n","# retain what we need\n","basedata=basedata.drop(['review_id',\n","                        'review_answer_timestamp',\n","                        'customer_zip_code_prefix',\n","                        'shipping_limit_date',\n","                        'seller_zip_code_prefix', 'seller_city', 'order_delivered_carrier_date',\n","                        'customer_city','review_comment_message', 'review_creation_date', 'review_comment_title',\n","                        'product_weight_g',\n","                        ], axis=1)\n","\n","basedata['order_delivered_customer_date'] = pd.to_datetime(basedata['order_delivered_customer_date'], format = \"%Y-%m-%d %H:%M:%S\")\n","basedata['order_estimated_delivery_date'] = pd.to_datetime(basedata['order_estimated_delivery_date'], format = \"%Y-%m-%d %H:%M:%S\")"],"metadata":{"id":"zvU5agA639-r","executionInfo":{"status":"ok","timestamp":1752143711217,"user_tz":-60,"elapsed":3191,"user":{"displayName":"Basil Basil","userId":"10665298970573720638"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Feature Engineering"],"metadata":{"id":"jby0qrAx4Nik"}},{"cell_type":"code","source":["# add Time to receive from estimate & Time to review columns\n","basedata['Time to receive from estimate'] = (basedata['order_delivered_customer_date'] - basedata['order_estimated_delivery_date']).dt.total_seconds()\n","\n","basedata['order_purchase_timestamp'] = pd.to_datetime(basedata['order_purchase_timestamp'], format = \"%Y-%m-%d %H:%M:%S\")\n","basedata['order_approved_at'] = pd.to_datetime(basedata['order_approved_at'],format = \"%Y-%m-%d %H:%M:%S\")\n","basedata.loc[:,'approv_duration'] = ( basedata['order_approved_at'] - basedata['order_purchase_timestamp'] ).dt.total_seconds()\n","\n","#seller state column formation\n","seller_state_counts = basedata['seller_state'].value_counts()\n","basedata['countstate'] = basedata['seller_state'].map(seller_state_counts)\n","basedata['newseller_state'] = np.where(basedata['countstate']>1000, basedata['seller_state'],'others')\n","\n","#cust state column formation\n","cust_state_counts = basedata['customer_state'].value_counts()\n","basedata['cust_countstate'] = basedata['customer_state'].map(cust_state_counts)\n","basedata['newcust_state'] = np.where(basedata['cust_countstate']>1000, basedata['customer_state'],'others')\n","\n","#prod state column formation\n","prod_state_counts = basedata['product_category_name'].value_counts()\n","basedata['prod_count'] = basedata['product_category_name'].map(prod_state_counts)\n","basedata['newproduct_cat'] = np.where(basedata['prod_count']>7500, basedata['product_category_name'],'unspecified')\n","\n","duplicates = basedata[basedata.duplicated(subset=['order_id', 'order_item_id'], keep=False)]\n","\n","#calculating the total payment, total freight, and total price.\n","basedata2=basedata\n","basedata2=basedata2[~basedata2.duplicated(subset=['order_id','order_item_id', 'payment_sequential', 'payment_value'],keep=False)] #removing duplicates becaus of left joins\n","basedata2['total_payment']= basedata2.groupby(['order_id','order_item_id'])['payment_value'].transform('sum') #totaling the payment value\n","basedata2['total_price']= basedata2.groupby(['order_id'])['price'].transform('sum') #sum of price\n","basedata2['total_freight']= basedata2.groupby(['order_id'])['freight_value'].transform('sum') #sum of freight value\n","\n","basedata2.loc[:,'payment_dif'] = basedata2['total_payment']-basedata2['total_price']-basedata2['total_freight'] #payment difference check\n","payment_dif_check2 = basedata2[basedata2['payment_dif'] > 1]\n","\n","basedata2dupes=basedata[basedata.duplicated(subset=['order_id','order_item_id', 'payment_sequential', 'payment_value'],keep=False)] #dupes removal chekcing\n","\n","#invalid delivery (has a delivery date but the status is not delivered)\n","invalid_deliv_rows = basedata2[(basedata2['order_status'] != 'delivered') & (basedata2['order_delivered_customer_date'].notnull())]\n","basedata2 = basedata2.drop(invalid_deliv_rows.index).reset_index(drop=True)\n","\n","#has no reviews\n","no_reviews = basedata2[basedata2['review_score'].isnull()]\n","\n","#calculating the difference between the estimated delivery date againts the delivered date\n","basedata2.loc[:,'estimated-delivered'] = basedata2['order_estimated_delivery_date'] - basedata2['order_delivered_customer_date']\n","basedata2.loc[:,'estimated-delivered-secs'] = basedata2.loc[:,'estimated-delivered'].dt.total_seconds()\n","\n","#calculating the approval duration (purchase_timestamp - approved_at)\n","basedata2['order_purchase_timestamp'] = pd.to_datetime(basedata2['order_purchase_timestamp'], format = \"%Y-%m-%d %H:%M:%S\")\n","basedata2['order_approved_at'] = pd.to_datetime(basedata2['order_approved_at'],format = \"%Y-%m-%d %H:%M:%S\")\n","basedata2.loc[:,'approv_duration'] = ( basedata2['order_approved_at'] - basedata2['order_purchase_timestamp'] ).dt.total_seconds()\n","\n","#calculating the freight ratio\n","basedata2.loc[:,'freight ratio'] = basedata2['total_freight']/(basedata2['total_price'])\n","\n","#removing invalid deliveries, no reviews, and duplicates\n","basedata2 = basedata2.drop(invalid_deliv_rows.index).reset_index(drop=True)\n","basedata2 = basedata2.drop(no_reviews.index).reset_index(drop = True)\n","basedata2=basedata2[~basedata2.duplicated(subset=['order_id'],keep=False)] #removing duplicates becaus of left joins\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GrfPwejI4PFp","executionInfo":{"status":"ok","timestamp":1752143712753,"user_tz":-60,"elapsed":1532,"user":{"displayName":"Basil Basil","userId":"10665298970573720638"}},"outputId":"40f62187-fa2c-4d18-842c-32abc7a9d8c5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3-2323592359.py:28: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  basedata2['total_payment']= basedata2.groupby(['order_id','order_item_id'])['payment_value'].transform('sum') #totaling the payment value\n","/tmp/ipython-input-3-2323592359.py:29: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  basedata2['total_price']= basedata2.groupby(['order_id'])['price'].transform('sum') #sum of price\n","/tmp/ipython-input-3-2323592359.py:30: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  basedata2['total_freight']= basedata2.groupby(['order_id'])['freight_value'].transform('sum') #sum of freight value\n","/tmp/ipython-input-3-2323592359.py:32: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  basedata2.loc[:,'payment_dif'] = basedata2['total_payment']-basedata2['total_price']-basedata2['total_freight'] #payment difference check\n"]}]},{"cell_type":"markdown","source":["# Modelling"],"metadata":{"id":"X5I4gPku4VHE"}},{"cell_type":"code","source":["# for the model\n","df = basedata2[['order_status','review_score','Time to receive from estimate','approv_duration','total_payment', 'total_price','freight ratio','product_description_lenght',\n","'product_photos_qty','newproduct_cat' ]]\n","\n","df = df[df['order_status'] == 'delivered']\n","df['review_score'] = np.where(df['review_score'].isin([4, 5]), 1,0)\n","\n","df=df.drop('order_status', axis=1)\n","df[['product_description_lenght', 'product_photos_qty']] = df[['product_description_lenght', 'product_photos_qty']].fillna(0) #we assume NA means 0 as there is no 0 value for both desc and photo qty\n","df = df.dropna() #dropping the other NA values\n","\n","#dummies product cat\n","df_dummies4 = pd.get_dummies(df['newproduct_cat'], prefix='productcat_',dtype=\"int\",drop_first=True)\n","# Append the dummies back to the original DataFrame\n","df = pd.concat([df, df_dummies4], axis=1)\n","df=df.drop('newproduct_cat', axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1FlvbOtH4Wwl","executionInfo":{"status":"ok","timestamp":1752143712837,"user_tz":-60,"elapsed":78,"user":{"displayName":"Basil Basil","userId":"10665298970573720638"}},"outputId":"959105b2-8503-457b-a0db-2ac389da8a2e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4-1924862702.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['review_score'] = np.where(df['review_score'].isin([4, 5]), 1,0)\n"]}]},{"cell_type":"markdown","source":["Random Forest"],"metadata":{"id":"D9uxgvN54eep"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression as LogR\n","from sklearn.ensemble import RandomForestClassifier as RF\n","from sklearn.ensemble import GradientBoostingClassifier as GBDT\n","from xgboost import XGBClassifier as XGB\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","y_value = df['review_score'] # set the y\n","y_values = np.ravel(y_value) # change to an array (list)\n","\n","x_values = df.drop('review_score', axis=1) # drop the y from the dataframe\n","\n","# split data into training and test\n","from sklearn.model_selection  import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(x_values, y_value, test_size = 0.2, random_state=4567, stratify=y_value)\n","\n","# print the shapes to check everything is OK\n","print(X_train.shape)\n","print(X_test.shape)\n","print(Y_train.shape)\n","print(Y_test.shape)\n","\n","total_samples = len(Y_train)\n","weight_class_0 = total_samples / np.sum(Y_train == 0)  # Weight for class 0\n","weight_class_1 = total_samples / np.sum(Y_train == 1)  # Weight for class 1\n","\n","RF_algo = RF()\n","RF_model = RF_algo.fit(X_train, Y_train)\n","\n","GBDT_algo = GBDT()\n","GBDT_model = GBDT_algo.fit(X_train, Y_train)\n","\n","XGB_algo = XGB()\n","XGB_model = XGB_algo.fit(X_train, Y_train)\n","\n","\n","from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n","from scipy.stats import uniform, randint\n","from sklearn.utils.class_weight import compute_sample_weight\n","\n","def random_search(algo, hyperparameters, X_train, Y_train):\n","  # do the search using 5 folds/chunks\n","  clf = RandomizedSearchCV(algo, hyperparameters, cv=5, random_state=2015,\n","                          scoring='precision_macro', n_iter=20, refit=True)\n","\n","  # pass the data to fit/train\n","  clf.fit(X_train, Y_train)\n","\n","  return clf.best_params_\n","\n","# RF\n","RF_tuned_parameters = {\n","    'n_estimators': randint(50, 500), # Draw from a uniform distribution between 50 and 500\n","    'max_depth': randint(2, 7),  # Draw from a uniform distribution between 2 and 7\n","    'min_samples_split': randint(2, 7),  # Draw from a uniform distribution between 2 and 7\n","    'max_features': ['sqrt', 'log2', None]\n","}\n","\n","RF_best_params = random_search(RF(class_weight='balanced'), RF_tuned_parameters, X_train, Y_train)\n","\n","# GBDT\n","GBDT_tuned_parameters = {\n","    'n_estimators': randint(25, 250), # Draw from a uniform distribution between 50 and 500\n","    'learning_rate': uniform(loc=0.01, scale=4.99),  # Draw from a uniform distribution between 0.01 and 5\n","    'criterion': ['friedman_mse', 'squared_error'],\n","    'max_depth': randint(2, 7)  # Draw from a uniform distribution between 2 and 7\n","}\n","\n","sample_weights = compute_sample_weight(class_weight='balanced', y=Y_train)\n","GBDT_best_params = random_search(GBDT(), GBDT_tuned_parameters, X_train, Y_train)\n","\n","\n","# XGBDT\n","XGB_tuned_parameters = {\n","    'n_estimators': randint(25, 250), # Draw from a uniform distribution between 50 and 500\n","    # eta is learning rate\n","    'eta': uniform(loc=0.01, scale=4.99),  # Draw from a uniform distribution between 0.01 and 5\n","    # objective is the same as criterion\n","    'objective': ['binary:logistic', 'binary:hinge'],\n","    'max_depth': randint(2, 7)  # Draw from a uniform distribution between 2 and 7\n","}\n","\n","scale_pos_weight = np.sum(Y_train == 0)/np.sum(Y_train == 1)\n","XGB_best_params = random_search(XGB(scale_pos_weight=scale_pos_weight), XGB_tuned_parameters, X_train, Y_train)\n","\n","\n","RF_algo = RF(**RF_best_params, class_weight= 'balanced')\n","RF_model = RF_algo.fit(X_train, Y_train)\n","\n","GBDT_algo = GBDT(**GBDT_best_params)\n","GBDT_model = GBDT_algo.fit(X_train, Y_train, sample_weight=sample_weights)\n","\n","XGB_algo = XGB(**XGB_best_params, scale_pos_weight = scale_pos_weight)\n","XGB_model = XGB_algo.fit(X_train, Y_train)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkwxqBL_4eEJ","outputId":"8e37a50c-43dc-4d51-d1a1-f0da5e6d03d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(61476, 12)\n","(15369, 12)\n","(61476,)\n","(15369,)\n"]}]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"ywrZPorv4qwx"}},{"cell_type":"code","source":["models = [RF_model, GBDT_model, XGB_model]\n","names = ['Random Forest', 'GBDT', 'XGBDT']\n","\n","for i in range(3):\n","  print(f\"Model: {names[i]}\")\n","\n","  # predict based on training data\n","  predict = models[i].predict(X_train)\n","\n","  # Calculate precision, recall, and F1-score\n","  precision, recall, f1_score, _ = precision_recall_fscore_support(Y_train, predict, average='macro')\n","  print(f\"Macro Precision: {precision}\")\n","  print(f\"Macro Recall: {recall}\")\n","  print(f\"Macro F1-score: {f1_score}\")\n","  print(\"\\n\")\n","\n","for i in range(3):\n","  print(f\"Model: {names[i]}\")\n","\n","  # predict based on training data\n","  predict = models[i].predict(X_train)\n","\n","  # Calculate precision, recall, and F1-score\n","  precision, recall, f1_score, _ = precision_recall_fscore_support(Y_train, predict, average='micro')\n","  print(f\"Micro Precision: {precision}\")\n","  print(f\"Micro Recall: {recall}\")\n","  print(f\"Micro F1-score: {f1_score}\")\n","  print(\"\\n\")\n","\n","for i in range(3):\n","  print(f\"Model: {names[i]}\")\n","\n","  # predict based on training data\n","  predict = models[i].predict(X_train)\n","\n","  # Calculate precision, recall, and F1-score\n","  precision, recall, f1_score, _ = precision_recall_fscore_support(Y_train, predict, average=None)\n","  print(f\"Class 1 Precision: {precision[1]}\")\n","  print(f\"Class 1 Recall: {recall[1]}\")\n","  print(f\"Class 1 F1-score: {f1_score[1]}\")\n","  print(\"\\n\")\n","\n","  print(f\"Class 0 Precision: {precision[0]}\")\n","  print(f\"Class 0 Recall: {recall[0]}\")\n","  print(f\"Class 0 F1-score: {f1_score[0]}\")\n","  print(\"\\n\")\n"],"metadata":{"id":"29fL24lb4nX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay as CM\n","\n","# Random Forest\n","print(\"Random Forest Confusion Matrix\")\n","predict = RF_model.predict(X_test)\n","CM.from_predictions(Y_test, predict)\n","\n","# GBDT\n","print(\"GBDT Confusion Matrix\")\n","predict = GBDT_model.predict(X_test)\n","print(CM.from_predictions(Y_test, predict))\n","\n","# XGB\n","print(\"XGB Confusion Matrix\")\n","predict = XGB_model.predict(X_test)\n","print(CM.from_predictions(Y_test, predict))\n","\n","\n","#%%\n","#random check\n","\n","items = items.groupby(['order_id'], as_index=False).agg({\n","    'price': 'sum',\n","    'freight_value': 'sum',\n","    'seller_id': 'first'\n","})\n","\n","items['freight_percent'] = (items['freight_value'] / items['price'])\n","\n","\n","check =basedata2.merge(items, how='left', on='order_id')\n","f_check2 = check[check['freight_percent']-check['freight ratio'] > 1]\n","\n","#%%\n","import time\n","from sklearn.inspection import permutation_importance\n","\n","start_time = time.time()\n","result = permutation_importance(\n","    RF_algo, X_test, Y_test, n_repeats=10, random_state=42, n_jobs=2\n",")\n","elapsed_time = time.time() - start_time\n","print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n","\n","feature_names = x_values.columns.tolist()\n","forest_importances = pd.Series(result.importances_mean, index=feature_names)\n","\n","importances = RF_algo.feature_importances_\n","forest_importances = pd.Series(importances, index=feature_names)\n","std = np.std([tree.feature_importances_ for tree in RF_algo.estimators_], axis=0)\n","\n","import matplotlib.pyplot as plt\n","fig, ax = plt.subplots()\n","forest_importances.plot.bar(yerr=std, ax=ax)\n","ax.set_title(\"Feature importances using MDI\")\n","ax.set_ylabel(\"Mean decrease in impurity\")\n","fig.tight_layout()\n","\n","fig, ax = plt.subplots()\n","forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n","ax.set_title(\"Feature importances using permutation on full model\")\n","ax.set_ylabel(\"Mean accuracy decrease\")\n","fig.tight_layout()\n","plt.show()\n","\n","\n","from xgboost import plot_importance\n","plot_importance(XGB_algo, max_num_features=10) # top 10 most important features\n","plt.show()\n","\n","\n","GBDT_importances = GBDT_algo.feature_importances_\n","GBDT_importances = pd.Series(GBDT_importances, index=X_train.columns)\n","#GBDTstd = np.std([tree.feature_importances_ for tree in GBDT_algo.estimators_], axis=0)\n","\n","\n","\n","fig, ax = plt.subplots(figsize=(10, 6))\n","GBDT_importances.plot.bar( ax=ax, capsize=4)\n","ax.set_title(\"GBDT Feature Importances using MDI\")\n","ax.set_ylabel(\"Mean Decrease in Impurity\")\n","ax.set_xlabel(\"Features\")\n","fig.tight_layout()\n","\n","plt.show()\n",""],"metadata":{"id":"ZldZV1sI4t34"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","def evaluate_model(model, X_train, Y_train, X_test, Y_test):\n","    print(f\"{model}\")\n","\n","    # Predict on training data\n","    train_preds = model.predict(X_train)\n","    train_precision = precision_score(Y_train, train_preds, average=None)[1]  # Precision for Class 1\n","    train_recall = recall_score(Y_train, train_preds, average='binary')  # Recall for Class 1 (binary classification)\n","    train_f1 = f1_score(Y_train, train_preds, average='binary')  # F1 for Class 1 (binary classification)\n","\n","    # Predict on test data\n","    test_preds = model.predict(X_test)\n","    test_precision = precision_score(Y_test, test_preds, average=None)[1]  # Precision for Class 1\n","    test_recall = recall_score(Y_test, test_preds, average='binary')  # Recall for Class 1 (binary classification)\n","    test_f1 = f1_score(Y_test, test_preds, average='binary')  # F1 for Class 1 (binary classification)\n","\n","    # Print results\n","    print(f\"Training - Precision (Class 1): {train_precision:.4f}, Recall (Class 1): {train_recall:.4f}, F1 (Class 1): {train_f1:.4f}\")\n","    print(f\"Test     - Precision (Class 1): {test_precision:.4f}, Recall (Class 1): {test_recall:.4f}, F1 (Class 1): {test_f1:.4f}\")\n","    print(\"\\n\")\n","\n","# Evaluate all models\n","models = [RF_model, GBDT_model, XGB_model]\n","for model in models:\n","    evaluate_model(model, X_train, Y_train, X_test, Y_test)"],"metadata":{"id":"fxMoU1II4z_K"},"execution_count":null,"outputs":[]}]}